{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from multiprocess import Process, Manager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrotick = 100\n",
    "sync_error = 0\n",
    "time_out = 4 * 60 * 60\n",
    "\n",
    "NUM_FLOW = 100000\n",
    "DATA_NAME = \"harmonic0\"\n",
    "TOPO_NAME = \"2\"\n",
    "\n",
    "task = pd.read_csv(\"../../data/utilization/utilization_10_10.csv\")\n",
    "network = pd.read_csv(\"../../data/utilization/utilization_topology.csv\")\n",
    "\n",
    "# task = pd.read_csv(\"../../dac_data/%s.csv\"%DATA_NAME)[:NUM_FLOW]\n",
    "# network = pd.read_csv(\"../../dac_data/%s_topology.csv\"%TOPO_NAME)\n",
    "for col in ['size','period','deadline','jitter']:\n",
    "    task[col] = np.ceil(task[col] / macrotick).astype(int)\n",
    "for col in ['t_proc','t_prop']:\n",
    "    network[col] = np.ceil(network[col] / macrotick).astype(int)\n",
    "    \n",
    "nodes = list(network['link'].apply(lambda x:eval(x)[0])) + \\\n",
    "    list(network['link'].apply(lambda x:eval(x)[1]))\n",
    "NODE_SET = list(set(nodes))\n",
    "ES_set = [x for x in NODE_SET if nodes.count(x) == 2]\n",
    "SW_set = list(set(NODE_SET) - set(ES_set))\n",
    "LCM = np.lcm.reduce(task['period'])\n",
    "net = np.zeros(shape = (max(NODE_SET) + 1, max(NODE_SET) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_var = {}\n",
    "\n",
    "for _, row in network.iterrows():\n",
    "    net_var.setdefault(eval(row['link'])[0], {})\n",
    "    net_var[eval(row['link'])[0]]['msd'] = row['t_proc']\n",
    "    net[eval(row['link'])[0], eval(row['link'])[1]] = 1\n",
    "\n",
    "## Create mapping from Link to index\n",
    "link_to_index = {}\n",
    "index_to_link = {}\n",
    "\n",
    "counter = 0\n",
    "for _, row in network.iterrows():\n",
    "    link = row['link']\n",
    "    link_to_index[link] = counter\n",
    "    index_to_link[counter] = link\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shortest path\n",
    "def bfs_paths(graph, start, goal):\n",
    "    queue = [(start, [start])]\n",
    "    while queue:\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        for _next in set(np.reshape(np.argwhere(graph[vertex] > 0),  -1)) - set(path):\n",
    "            if _next == goal:\n",
    "                yield path + [_next]\n",
    "            else:\n",
    "                queue.append((_next, path + [_next]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_attr = {}\n",
    "task_var = {}\n",
    "link_to_flow = {}\n",
    "next_link = {} ## {flow: {link: [next_link]}}\n",
    "pre_link = {} ## {flow: {link: [pre_link]}}\n",
    "for k, row in task.iterrows():\n",
    "    task_attr.setdefault(k, {})\n",
    "    task_var.setdefault(k, {})\n",
    "    \n",
    "    ## f_k\n",
    "    task_attr[k]['s'] = int(row['src'])\n",
    "    task_attr[k]['d'] = int(eval(row['dst'])[0])\n",
    "    task_attr[k]['ct'] = int(row['period'])\n",
    "    task_attr[k]['rsl'] = int(row['size'] * 8)\n",
    "    task_attr[k]['ml'] = int(row['deadline'])\n",
    "    task_attr[k]['q'] = 0\n",
    "    \n",
    "    task_attr[k]['route'] = next(bfs_paths(net, int(row['src']), int(eval(row['dst'])[0])))\n",
    "    \n",
    "    count = 0\n",
    "    next_link.setdefault(k, {})\n",
    "    pre_link.setdefault(k, {})\n",
    "    for a, b in zip(task_attr[k]['route'][:-1], task_attr[k]['route'][1:]):\n",
    "        task_var[k][str((a, b))] = [-1] * 3 ## [weight, queue, offset]\n",
    "        task_var[k][str((a, b))][0] = len(task_attr[k]['route']) * task_attr[k]['rsl'] / task_attr[k]['ct']\n",
    "        link_to_flow.setdefault(str((a, b)), [])\n",
    "        link_to_flow[str((a, b))].append(k)\n",
    "        if count < len(task_attr[k]['route']) - 2:\n",
    "            next_link[k][str((a, b))] = str((task_attr[k]['route'][count+1], task_attr[k]['route'][count + 2]))\n",
    "        else:\n",
    "            next_link[k][str((a, b))] = None\n",
    "        if count > 0:\n",
    "            pre_link[k][str((a, b))] = str((task_attr[k]['route'][count-1], task_attr[k]['route'][count]))\n",
    "        else:\n",
    "            pre_link[k][str((a, b))] = None\n",
    "        count += 1\n",
    "       \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_to_index = {}\n",
    "index_to_route = {}\n",
    "for k in task_attr:\n",
    "    route_to_index.setdefault(k, {})\n",
    "    index_to_route.setdefault(k, {})\n",
    "    for i, v in enumerate(task_attr[k]['route'][:-1]):\n",
    "        route_to_index[k][str((v, task_attr[k]['route'][i+1]))] = i\n",
    "        index_to_route[k][i] = str((v, task_attr[k]['route'][i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DivPhases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAllLinkPhased(link_set):\n",
    "    for link in link_set:\n",
    "        if not link[1]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper:\n",
    "\n",
    "The only condition for a link l i to be assigned to a phase l i .ϕ is that all frames transmitted through that link f ∈ F TT : f .s j .ζ = l i must have all previous links (links closer to destination) assigned to previous phases ∀f .s k .ζ : k < j|f .s k .ζ .ϕ < f .s j .ζ .ϕ.\n",
    "\n",
    "But from Order1 and Order2, it seems two links can also have dependency even neither one is not the other's previous link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dependency = {} ## {link, [depdent links...]}\n",
    "\n",
    "for i in task_attr:\n",
    "    for hop, link in enumerate(task_var[i].keys()):\n",
    "        link_dependency.setdefault(link, set())\n",
    "        if hop != len(task_var[i].keys()) - 1:\n",
    "            link_dependency[link].add(list(task_var[i].keys())[hop+1])\n",
    "            \n",
    "    # for j in task_attr:\n",
    "    #     if i < j:\n",
    "    #         for hop_i, link_i in enumerate(task_var[i].keys()):\n",
    "    #             for hop_j, link_j in enumerate(task_var[j].keys()):\n",
    "    #                 if link_i == link_j:\n",
    "    #                     link_ib = list(task_var[i].keys())[hop_i-1]\n",
    "    #                     link_jb = list(task_var[j].keys())[hop_j-1]\n",
    "    #                     link_dependency.setdefault(link_jb, set())\n",
    "    #                     link_dependency[link_jb].add(link_ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toposort(data):\n",
    "    data = {k: set(v) for k, v in data.items()}\n",
    "    graph = defaultdict(set)\n",
    "    nodes = set()\n",
    "    for k, v in data.items():\n",
    "        graph[k] = v\n",
    "        nodes.add(k)\n",
    "        nodes.update(v)\n",
    "\n",
    "    result = []\n",
    "    while nodes:\n",
    "        no_dep = set(n for n in nodes if not graph[n])\n",
    "        if not no_dep:\n",
    "            raise Exception('Cyclic dependencies exist among these items: {}'.format(', '.join(nodes)))\n",
    "        nodes.difference_update(no_dep)\n",
    "        result.append(no_dep)\n",
    "\n",
    "        for node, edges in graph.items():\n",
    "            edges.difference_update(no_dep)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: {'(7, 15)', '(3, 11)', '(1, 9)', '(4, 12)', '(6, 14)', '(5, 13)', '(0, 8)', '(2, 10)'}\n",
      "Phase 2: {'(6, 7)', '(1, 6)', '(2, 5)', '(3, 4)', '(0, 7)', '(5, 4)'}\n",
      "Phase 3: {'(6, 5)', '(2, 3)', '(1, 0)', '(5, 6)'}\n",
      "Phase 4: {'(7, 6)', '(2, 1)', '(4, 5)', '(1, 2)'}\n",
      "Phase 5: {'(10, 2)', '(0, 1)', '(9, 1)', '(6, 1)', '(3, 2)', '(5, 2)'}\n",
      "Phase 6: {'(13, 5)', '(11, 3)', '(8, 0)', '(14, 6)', '(4, 3)', '(7, 0)'}\n",
      "Phase 7: {'(12, 4)', '(15, 7)'}\n"
     ]
    }
   ],
   "source": [
    "result = toposort(link_dependency)\n",
    "for i, nodes in enumerate(result, start=1):\n",
    "    print(f\"Phase {i}: {nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision(link, flow, offset, scheduled_frame):\n",
    "    global task_attr\n",
    "    w_i = task_attr[flow]['rsl']\n",
    "    p_i = task_attr[flow]['ct']\n",
    "    o_i = offset\n",
    "    same_link_frames = scheduled_frame[link]\n",
    "    for flow_j, sche in same_link_frames.items():\n",
    "        w_j = task_attr[flow_j]['rsl']\n",
    "        p_j = task_attr[flow_j]['ct']\n",
    "        o_j = sche[1]\n",
    "        lcm = np.lcm(p_i, p_j)\n",
    "        for u, v in [(u, v) \n",
    "                     for u in range(0, int(lcm / p_i))\n",
    "                     for v in range(0, int(lcm / p_j))]:\n",
    "            if (o_j + v * p_j <= o_i + u * p_i + w_i ) and (o_i + u * p_i <= o_j + v * p_j + w_j):\n",
    "                return True\n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_order_violate_set(link, flow, offset, scheduled_frame) -> list:\n",
    "    global task_attr, next_link, pre_link\n",
    "    \n",
    "    violate_set = []\n",
    "    q_i = task_attr[flow]['q']\n",
    "    o_i = offset\n",
    "\n",
    "    ## Next link must be already scheduled\n",
    "    next_link_i = next_link[flow][link]\n",
    "    if next_link_i == None:\n",
    "        return violate_set\n",
    "    \n",
    "    on_i = scheduled_frame[next_link_i][flow][1]\n",
    "    \n",
    "    ## Find other flows scheduled on the next link\n",
    "    for next_link_j in scheduled_frame:\n",
    "        if next_link_i == next_link_j and link in scheduled_frame:\n",
    "            for flow_j in scheduled_frame[next_link_j]:\n",
    "                if flow_j != flow:\n",
    "                    q_j = task_attr[flow_j]['q']\n",
    "                    if q_j == q_i:\n",
    "                        link_j = pre_link[flow_j][next_link_j]\n",
    "                        if link_j == None:\n",
    "                            continue\n",
    "                        if link_j not in scheduled_frame:\n",
    "                            continue\n",
    "                        if flow_j not in scheduled_frame[link_j]:\n",
    "                            continue\n",
    "                        o_j = scheduled_frame[link_j][flow_j][1]\n",
    "                        on_j = scheduled_frame[next_link_j][flow_j][1]\n",
    "                        \n",
    "                        violate_set.append([\n",
    "                            flow,\n",
    "                            flow_j,\n",
    "                            o_i, \n",
    "                            on_i,\n",
    "                            o_j,\n",
    "                            on_j\n",
    "                        ])\n",
    "    return violate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order1(i, j, o_i, on_i, o_j, on_j):\n",
    "    global task_attr\n",
    "    p_i, p_j = task_attr[i]['ct'], task_attr[j]['ct']\n",
    "    lcm = np.lcm(p_i, p_j)\n",
    "    for u, v in [(u, v) \n",
    "                     for u in range(0, int(lcm / p_i))\n",
    "                     for v in range(0, int(lcm / p_j))]:\n",
    "        if (o_j + v * p_j) < (o_i + u * p_i) and (on_j + v * p_j) > (on_i + u * p_i):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def order2(i, j,  o_i, on_i, o_j, on_j):\n",
    "    global task_attr\n",
    "    p_i, p_j = task_attr[i]['ct'], task_attr[j]['ct']\n",
    "    lcm = np.lcm(p_i, p_j)\n",
    "    for u, v in [(u, v) \n",
    "                     for u in range(0, int(lcm / p_i))\n",
    "                     for v in range(0, int(lcm / p_j))]:\n",
    "        if (o_j + v * p_j) > (o_i + u * p_i) and (on_j + v * p_j) < (on_i + u * p_i):\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_link(link, scheduled_frame):\n",
    "    scheduled_frame = copy.deepcopy(scheduled_frame)\n",
    "    flag = 3\n",
    "    for i in link_to_flow[link]:\n",
    "        next_link_i = next_link[i][link]\n",
    "        if next_link_i == None:\n",
    "            offset = task_attr[i]['ml']\n",
    "        else:\n",
    "            offset = min(scheduled_frame[next_link_i][i][1] - task_attr[i]['rsl'] - net_var[eval(link)[0]]['msd'], task_attr[i]['ml'])\n",
    "        \n",
    "        while flag:\n",
    "            if link in scheduled_frame and collision(link, i, offset, scheduled_frame):\n",
    "                offset -= 1\n",
    "                flag = 3 ## offset - 1\n",
    "                continue\n",
    "            \n",
    "            collision_set = get_potential_order_violate_set(link, i, offset, scheduled_frame)\n",
    "            for flow, flow_j, offset, on_i, o_j,on_j in collision_set:\n",
    "                if order1(flow, flow_j, offset,  on_i, o_j, on_j):\n",
    "                    if task_attr[i]['q'] == 8:\n",
    "                        flag = 1\n",
    "                        break\n",
    "                    else:\n",
    "                        flag = 2 ## queue + 1\n",
    "                        break\n",
    "                if order2(flow, flow_j, offset,  on_i, o_j, on_j):\n",
    "                    if task_attr[i]['q'] == 8:\n",
    "                        flag = 0 ## failed\n",
    "                        break\n",
    "                    else:\n",
    "                        flag = 2 ## queue + 1\n",
    "                        break\n",
    "            \n",
    "            if offset < 0:\n",
    "                flag = 0 ## failed\n",
    "            \n",
    "            if flag == 0:\n",
    "                print(\"Failed\")\n",
    "                break\n",
    "            elif flag == 1:\n",
    "                offset -= 1\n",
    "                flag = 3\n",
    "                continue\n",
    "            elif flag == 2:\n",
    "                task_attr[i]['q'] += 1\n",
    "                flag = 3\n",
    "                continue\n",
    "            elif flag == 3:\n",
    "                scheduled_frame.setdefault(link, {})\n",
    "                scheduled_frame[link][i] = [task_attr[i]['q'], offset]\n",
    "                # print(f\"{i}-{link} {[task_attr[i]['q'], offset]}\")\n",
    "                break\n",
    "    return scheduled_frame, flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(dict1, dict2):\n",
    "    dict1 = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if key in dict1:\n",
    "            dict1[key].update(value)\n",
    "        else:\n",
    "            dict1[key] = value\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search should starts from the maximum offset\n",
    "# # Manually create 4 processes for seaching\n",
    "\n",
    "# ## scheduled_frame = {link: {flow: [queue, offset]}}\n",
    "\n",
    "# scheduled_frame = {}\n",
    "\n",
    "\n",
    "# for phase in range(len(result)):\n",
    "#     ## [TODO] Make this parallel\n",
    "#     print(\"Phase: \", phase)\n",
    "#     phase_result_list = []\n",
    "#     ## Create 4 processes for searching\n",
    "#     for link in result[phase]:\n",
    "#         schedule, flag = schedule_link(link, scheduled_frame)\n",
    "#         if flag == 0:\n",
    "#             print(\"Failed\")\n",
    "#         phase_result_list.append(schedule)\n",
    "#     for schedule in phase_result_list:\n",
    "#         scheduled_frame = merge_dict(scheduled_frame, schedule)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase:  0\n",
      "Phase:  1\n",
      "Phase:  2\n",
      "Phase:  3\n",
      "Phase:  4\n",
      "Phase:  5\n",
      "Phase:  6\n"
     ]
    }
   ],
   "source": [
    "scheduled_frame = {}\n",
    "\n",
    "def schedule_and_update(link, scheduled_frame, result_dict):\n",
    "    schedule, flag = schedule_link(link, scheduled_frame)\n",
    "    if flag == 0:\n",
    "        print(\"Failed\")\n",
    "    else:\n",
    "        result_dict[link] = schedule\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for phase in range(len(result)):\n",
    "        print(\"Phase: \", phase)\n",
    "        \n",
    "        with Manager() as manager:\n",
    "            result_dict = manager.dict()\n",
    "            processes = []\n",
    "\n",
    "            for link in result[phase]:\n",
    "                # If 4 processes are already running, wait until one has finished\n",
    "                while len(processes) >= 4:\n",
    "                    for p in processes:\n",
    "                        if not p.is_alive():\n",
    "                            p.join()\n",
    "                            processes.remove(p)\n",
    "                    time.sleep(0.1)  # Optional short sleep to prevent excessive CPU usage\n",
    "\n",
    "                # Start a new process\n",
    "                p = Process(target=schedule_and_update, args=(link, scheduled_frame, result_dict))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "\n",
    "            # Ensure all processes have finished execution\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "\n",
    "            # Update the main scheduled_frame dict\n",
    "            for link, schedule in result_dict.items():\n",
    "                scheduled_frame = merge_dict(scheduled_frame, schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for link, flows in scheduled_frame.items():\n",
    "    for flow in flows:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for flow in task_attr:\n",
    "    for link in task_attr[flow]['route']:\n",
    "        count += 1\n",
    "    count -= 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
