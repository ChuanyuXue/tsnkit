{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import os\n",
    "np.random.seed(1024)\n",
    "\n",
    "# NUM_STREAM_SPEC = list(range(8, 188, 10))\n",
    "NUM_STREAM_SPEC = [10, 40, 70, 100, 130, 160, 190, 220]\n",
    "\n",
    "ERROR = 2_000\n",
    "\n",
    "PERIOD_SPEC = [1, 2, 3, 4, 5, 6]\n",
    "def period_spec(opt):\n",
    "    if opt == 1:\n",
    "        return 2_000_000\n",
    "    if opt == 2:\n",
    "        return 400_000\n",
    "    if opt == 3:\n",
    "        return int(np.random.choice([500_000, 1_000_000, 2_000_000, 4_000_000]))\n",
    "    if opt == 4:\n",
    "        return int(np.random.choice([100_000, 200_000, 400_000, 800_000]))\n",
    "    if opt == 5:\n",
    "        return int(np.random.choice([250_000, 500_000, 1_250_000, 2_500_000, 4_000_000]))\n",
    "    if opt == 6:\n",
    "        return int(np.random.choice([50_000, 100_000, 250_000, 500_000, 800_000]))\n",
    "    assert False, \"Invalid option\"\n",
    "\n",
    "SIZE_SPEC = [1,2,3,4,5]\n",
    "def data_spec(opt):\n",
    "    if opt == 1:\n",
    "        return 50\n",
    "    if opt == 2:\n",
    "        return int(np.random.choice(range(100, 501, 100)))\n",
    "    if opt == 3:\n",
    "        return int(np.random.choice(range(200, 1501, 100)))\n",
    "    if opt == 4:\n",
    "        return int(np.random.choice(range(500, 4501, 100)))\n",
    "    if opt == 5:\n",
    "        return int(np.random.choice(range(1500, 4501, 100)))\n",
    "    assert False, \"Invalid option\"\n",
    "\n",
    "DEADLINE_SPEC = [1,2,3,4,5]\n",
    "def deadline_spec(opt):\n",
    "    if opt == 1:\n",
    "        assert False\n",
    "    if opt == 2:\n",
    "        return int(np.random.choice([100_000, 200_000, 400_000, 800_000, 1_600_000]))\n",
    "    if opt == 3:\n",
    "        return int(np.random.choice([10_000, 25_000, 50_000, 100_000, 200_000, 400_000]))\n",
    "    if opt == 4:\n",
    "        return int(np.random.choice([0, 10_000, 20_000, 25_000, 50_000]))\n",
    "    if opt == 5:\n",
    "        return 0\n",
    "    assert False, \"Invalid option\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_paths(graph, start, goal):\n",
    "    return nx.shortest_path(graph, start, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(num_sw, num_queue, data_rate, header):\n",
    "    num_node = num_sw * 2\n",
    "    net = np.zeros(shape = (num_node, num_node))\n",
    "\n",
    "    ## Connect the line\n",
    "    for i in range(0, num_sw - 1):\n",
    "        net[i, i+1] = 1\n",
    "        net[i+1, i] = 1\n",
    "    ## Connect the switch and the end-station\n",
    "    for i in range(num_sw):\n",
    "        net[i+num_sw, i] = 1\n",
    "        net[i, i+num_sw] = 1\n",
    "\n",
    "    result = []\n",
    "    for i in range(num_node):\n",
    "        for j in range(num_node):\n",
    "            if net[i][j]:\n",
    "                link = []\n",
    "                link.append((i, j))\n",
    "                link.append(num_queue)\n",
    "                link.append(data_rate)\n",
    "                link.append(ERROR)\n",
    "                link.append(0)\n",
    "                result.append(link)\n",
    "\n",
    "    result = pd.DataFrame(result, columns=['link','q_num','rate','t_proc','t_prop'])\n",
    "    result.to_csv(header + '.csv', index=False)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring(num_sw, num_queue, data_rate, header):\n",
    "    num_node = num_sw * 2\n",
    "    net = np.zeros(shape = (num_node, num_node))\n",
    "\n",
    "    ## Connect the line\n",
    "    for i in range(0, num_sw - 1):\n",
    "        net[i, i+1] = 1\n",
    "        net[i+1, i] = 1\n",
    "    ## Connect the switch and the end-station\n",
    "    for i in range(num_sw):\n",
    "        net[i+num_sw, i] = 1\n",
    "        net[i, i+num_sw] = 1\n",
    "    \n",
    "    ## Connect the ring\n",
    "    net[0, num_sw - 1] = 1\n",
    "    net[num_sw - 1, 0] = 1\n",
    "\n",
    "    result = []\n",
    "    for i in range(num_node):\n",
    "        for j in range(num_node):\n",
    "            if net[i][j]:\n",
    "                link = []\n",
    "                link.append((i, j))\n",
    "                link.append(num_queue)\n",
    "                link.append(data_rate)\n",
    "                link.append(ERROR)\n",
    "                link.append(0)\n",
    "                result.append(link)\n",
    "\n",
    "    result = pd.DataFrame(result, columns=['link','q_num','rate','t_proc','t_prop'])\n",
    "    result.to_csv(header + '.csv', index=False)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(num_sw, num_queue, data_rate, header):\n",
    "    num_node = num_sw * 2 + 1\n",
    "    net = np.zeros(shape = (num_node, num_node))\n",
    "\n",
    "    for i in range(num_sw):\n",
    "        net[i, i * 2 + 1] = 1\n",
    "        net[i * 2 + 1, i] = 1\n",
    "        net[i, i * 2 + 2] = 1\n",
    "        net[i * 2 + 2, i] = 1\n",
    "    result = []\n",
    "    for i in range(num_node):\n",
    "        for j in range(num_node):\n",
    "            if net[i][j]:\n",
    "                link = []\n",
    "                link.append((i, j))\n",
    "                link.append(num_queue)\n",
    "                link.append(data_rate)\n",
    "                link.append(ERROR)\n",
    "                link.append(0)\n",
    "                result.append(link)\n",
    "\n",
    "    result = pd.DataFrame(result, columns=['link','q_num','rate','t_proc','t_prop'])\n",
    "    result.to_csv(header + '.csv', index=False)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh(num_sw, num_queue, data_rate, header):\n",
    "    num_node = num_sw * 2\n",
    "    net = np.zeros(shape = (num_node, num_node))\n",
    "    \n",
    "    ## Connect the line\n",
    "    for i in range(0, num_sw - 1):\n",
    "        net[i, i+1] = 1\n",
    "        net[i+1, i] = 1\n",
    "    ## Connect the switch and the end-station\n",
    "    for i in range(num_sw):\n",
    "        net[i+num_sw, i] = 1\n",
    "        net[i, i+num_sw] = 1\n",
    "    \n",
    "    ## Connect the mesh\n",
    "    net[0, num_sw - 1] = 1\n",
    "    net[num_sw - 1, 0] = 1\n",
    "\n",
    "    ## Connect sw on the ring like DNA\n",
    "    for i in range(0, num_sw // 2):\n",
    "        net[i, num_sw - i - 1] = 1\n",
    "        net[num_sw - i - 1, i] = 1\n",
    "\n",
    "    result = []\n",
    "    for i in range(num_node):\n",
    "        for j in range(num_node):\n",
    "            if net[i][j]:\n",
    "                link = []\n",
    "                link.append((i, j))\n",
    "                link.append(num_queue)\n",
    "                link.append(data_rate)\n",
    "                link.append(ERROR)\n",
    "                link.append(0)\n",
    "                result.append(link)\n",
    "\n",
    "    result = pd.DataFrame(result, columns=['link','q_num','rate','t_proc','t_prop'])\n",
    "    result.to_csv(header + '.csv', index=False)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_func = [line, ring, tree, mesh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flowset(graph, size_param, period_param, deadline_param, num_thres_param, num_sw, num_es, header):\n",
    "    result = []\n",
    "    i = 0\n",
    "    uti = 0\n",
    "    uti_ports = np.zeros(num_es)\n",
    "    while True:\n",
    "        if i >= num_thres_param:\n",
    "            result = pd.DataFrame(result, columns = ['id','src','dst','size','period','deadline','jitter'])\n",
    "            result.to_csv(header + '.csv', index=False)\n",
    "            return\n",
    "\n",
    "        availble_es = np.argwhere(uti_ports <= 0.75).reshape(-1)\n",
    "        if availble_es.size == 0:\n",
    "            availble_es = np.array([x for x in range(num_es)])\n",
    "        \n",
    "        start = int(np.random.choice(availble_es + num_sw))\n",
    "        end = int(np.random.choice([x for x in range(num_sw, num_sw + num_es) if x != start]))\n",
    "        path = bfs_paths(graph, start, end)\n",
    "\n",
    "        period = period_spec(period_param)\n",
    "        size = data_spec(size_param)\n",
    "        deadline = (len(path) - 1) * (ERROR + size * 8) + deadline_spec(deadline_param) if deadline_param > 1 else period\n",
    "        if deadline <= period:\n",
    "            result.append([i, start, [end], size, period, deadline, deadline])\n",
    "            uti += size * 8 / period\n",
    "            uti_ports[start - num_sw] += size * 8 / period\n",
    "            i += 1\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_generator(ins, max_ins = 9999999):\n",
    "    global SIZE_SPEC, PERIOD_SPEC, DEADLINE_SPEC, NUM_STREAM_SPEC\n",
    "\n",
    "    if not os.path.exists(f'grid/{ins}'):\n",
    "        os.makedirs(f'grid/{ins}')\n",
    "        print(f\"Subfolder {ins} created successfully.\")   \n",
    "    else:\n",
    "        print(f\"Subfolder {ins} already exists.\")\n",
    "\n",
    "    total = len(SIZE_SPEC) * len(PERIOD_SPEC) * len(DEADLINE_SPEC) * len(NUM_STREAM_SPEC) * 8 * 4\n",
    "    count = ins * total\n",
    "    dataset_logs = []\n",
    "    with tqdm(total=total, desc=f\"Process {ins}\", position=ins) as pbar:\n",
    "        try:\n",
    "            for size in SIZE_SPEC:\n",
    "                for period in PERIOD_SPEC:\n",
    "                    for deadline in DEADLINE_SPEC:\n",
    "                        for num_thres in NUM_STREAM_SPEC:\n",
    "                            for num_sw in range(8, 88, 10):\n",
    "                                for topo in range(4):\n",
    "                                    header = f'grid/{ins}/' + str(count)\n",
    "                                    net = topo_func[topo](num_sw, num_queue = 8, data_rate=1, header=header + '_topo')\n",
    "                                    generate_flowset(nx.DiGraph(net), size, period, deadline, num_thres, num_sw, num_sw, header + '_task')\n",
    "\n",
    "                                    exp_info = [count, ins, size, period, deadline, topo, num_thres, num_sw]\n",
    "                                    dataset_logs.append(exp_info)\n",
    "\n",
    "                                    count += 1\n",
    "                                    pbar.update(1)\n",
    "                                \n",
    "                                    if count >= max_ins:\n",
    "                                        raise StopIteration\n",
    "        except StopIteration:\n",
    "            pass\n",
    "                            \n",
    "    exp_logs = pd.DataFrame(dataset_logs, columns = ['id','ins','size','period','deadline','topo','num_thres','num_sw'])\n",
    "    exp_logs.to_csv(f'grid/{ins}/dataset_logs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists.\n",
      "Subfolder 0 already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ced054f661844488d9e6c23fcb90a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 0:   0%|          | 0/38400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ## Generate the test data\n",
    "    # header = \"test\"\n",
    "    # num_bridge = 10\n",
    "    # num_stream = 10\n",
    "    # net = topo_func[0](num_bridge, num_queue = 8, data_rate=1, header=header + '_topo')\n",
    "    # generate_flowset(nx.DiGraph(net), 2, 1, 1, num_stream, num_bridge, num_bridge, header + '_task')\n",
    "    \n",
    "    # Generate the dataset used in paper\n",
    "    if not os.path.exists('grid'):\n",
    "        os.makedirs('grid')\n",
    "        print(\"Folder created successfully.\")\n",
    "    else:\n",
    "        print(\"Folder already exists.\")\n",
    "\n",
    "    grid_generator(0, 1000)\n",
    "    ## Generate the grid topology in parallel\n",
    "    # with Pool(4) as p:\n",
    "    #     p.map(grid_generator, range(4))\n",
    "    # p.close()\n",
    "    # p.join()\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins0_directory = pd.read_csv('grid/0/dataset_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7d74ffaa694a2e8059f59135fd34ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/38400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ins0_task = {}\n",
    "ins0_topo = {}\n",
    "\n",
    "for i, row in tqdm(ins0_directory.iterrows(), total=len(ins0_directory), desc=\"Loading data\"):\n",
    "    ins0_task[row['id']] = pd.read_csv(f\"grid/0/{row['id']}_task.csv\")\n",
    "    ins0_topo[row['id']] = pd.read_csv(f\"grid/0/{row['id']}_topo.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 1 has 883200 flows, with average size 50.0 and std 0.0\n",
      "Size 2 has 883200 flows, with average size 298.64639945652175 and std 141.3152014230948\n",
      "Size 3 has 883200 flows, with average size 835.5451766304348 and std 403.07598215838766\n",
      "Size 4 has 883200 flows, with average size 2360.315330615942 and std 1181.5430307983695\n",
      "Size 5 has 883200 flows, with average size 2919.305366847826 and std 893.6494976446756\n"
     ]
    }
   ],
   "source": [
    "for size in SIZE_SPEC:\n",
    "    spec_id = ins0_directory[ins0_directory['size'] == size]['id'].values\n",
    "    all_size = []\n",
    "    for id in spec_id:\n",
    "        all_size.append(ins0_task[id]['size'].values)\n",
    "    all_size = np.concatenate(all_size)\n",
    "    print(f\"Size {size} has {len(all_size)} flows, with average size {np.mean(all_size)} and std {np.std(all_size)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period 1 has 736000 flows, with average period 2000000.0 and std 0.0\n",
      "Period 2 has 736000 flows, with average period 400000.0 and std 0.0\n",
      "Period 3 has 736000 flows, with average period 1958218.0706521738 and std 1344960.346210985\n",
      "Period 4 has 736000 flows, with average period 476963.5869565217 and std 273931.4274320488\n",
      "Period 5 has 736000 flows, with average period 1887141.3043478262 and std 1391150.8863013366\n",
      "Period 6 has 736000 flows, with average period 473892.6630434783 and std 270613.2265246511\n"
     ]
    }
   ],
   "source": [
    "for period in PERIOD_SPEC:\n",
    "    spec_id = ins0_directory[ins0_directory['period'] == period]['id'].values\n",
    "    all_period = []\n",
    "    for id in spec_id:\n",
    "        all_period.append(ins0_task[id]['period'].values)\n",
    "    all_period = np.concatenate(all_period)\n",
    "    print(f\"Period {period} has {len(all_period)} flows, with average period {np.mean(all_period)} and std {np.std(all_period)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Check deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deadline 1 has 883200 flows, with average deadline 1115102.5249094204 and std 1099454.418758253\n",
      "Deadline 2 has 883200 flows, with average deadline 476389.58423913043 and std 430343.0203285485\n",
      "Deadline 3 has 883200 flows, with average deadline 232136.19701086957 and std 196622.23397128755\n",
      "Deadline 4 has 883200 flows, with average deadline 149805.76811594202 and std 159165.13861016821\n",
      "Deadline 5 has 883200 flows, with average deadline 129566.6875 and std 158966.88125838185\n"
     ]
    }
   ],
   "source": [
    "for deadline in DEADLINE_SPEC:\n",
    "    spec_id = ins0_directory[ins0_directory['deadline'] == deadline]['id'].values\n",
    "    all_deadline = []\n",
    "    for id in spec_id:\n",
    "        all_deadline.append(ins0_task[id]['deadline'].values)\n",
    "    all_deadline = np.concatenate(all_deadline)\n",
    "    print(f\"Deadline {deadline} has {len(all_deadline)} flows, with average deadline {np.mean(all_deadline)} and std {np.std(all_deadline)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check num-stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num stream 10 has 4800 problem instances, with average num stream 10.0 and std 0.0\n",
      "Num stream 40 has 4800 problem instances, with average num stream 40.0 and std 0.0\n",
      "Num stream 70 has 4800 problem instances, with average num stream 70.0 and std 0.0\n",
      "Num stream 100 has 4800 problem instances, with average num stream 100.0 and std 0.0\n",
      "Num stream 130 has 4800 problem instances, with average num stream 130.0 and std 0.0\n",
      "Num stream 160 has 4800 problem instances, with average num stream 160.0 and std 0.0\n",
      "Num stream 190 has 4800 problem instances, with average num stream 190.0 and std 0.0\n",
      "Num stream 220 has 4800 problem instances, with average num stream 220.0 and std 0.0\n"
     ]
    }
   ],
   "source": [
    "for num_stream in NUM_STREAM_SPEC:\n",
    "    spec_id = ins0_directory[ins0_directory['num_thres'] == num_stream]['id'].values\n",
    "    all_num_stream = []\n",
    "    for idd in spec_id:\n",
    "        all_num_stream.append(len(ins0_task[idd]))\n",
    "    print(f\"Num stream {num_stream} has {len(all_num_stream)} problem instances, with average num stream {np.mean(all_num_stream)} and std {np.std(all_num_stream)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Check number of links for each network scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sw 8 has 4800 problem instances, with average num sw 32.5 and std 2.179449471770337\n",
      "Num sw 18 has 4800 problem instances, with average num sw 75.0 and std 6.4031242374328485\n",
      "Num sw 28 has 4800 problem instances, with average num sw 117.5 and std 10.712142642814275\n",
      "Num sw 38 has 4800 problem instances, with average num sw 160.0 and std 15.033296378372908\n",
      "Num sw 48 has 4800 problem instances, with average num sw 202.5 and std 19.35846068260594\n",
      "Num sw 58 has 4800 problem instances, with average num sw 245.0 and std 23.68543856465402\n",
      "Num sw 68 has 4800 problem instances, with average num sw 287.5 and std 28.01338965566288\n",
      "Num sw 78 has 4800 problem instances, with average num sw 330.0 and std 32.341923257592455\n"
     ]
    }
   ],
   "source": [
    "for num_sw in range(8, 88, 10):\n",
    "    spec_id = ins0_directory[ins0_directory['num_sw'] == num_sw]['id'].values\n",
    "    all_num_sw = []\n",
    "    for idd in spec_id:\n",
    "        all_num_sw.append(len(ins0_topo[idd]))\n",
    "    print(f\"Num sw {num_sw} has {len(all_num_sw)} problem instances, with average links {np.mean(all_num_sw)} and std {np.std(all_num_sw)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Check number of links for each topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topo 0 has 9600 problem instances, with average links 170.0 and std 91.6515138991168\n",
      "Topo 1 has 9600 problem instances, with average links 172.0 and std 91.6515138991168\n",
      "Topo 2 has 9600 problem instances, with average links 172.0 and std 91.6515138991168\n",
      "Topo 3 has 9600 problem instances, with average links 211.0 and std 114.564392373896\n"
     ]
    }
   ],
   "source": [
    "for topo in range(4):\n",
    "    spec_id = ins0_directory[ins0_directory['topo'] == topo]['id'].values\n",
    "    all_topo = []\n",
    "    for idd in spec_id:\n",
    "        all_topo.append(len(ins0_topo[idd]))\n",
    "    print(f\"Topo {topo} has {len(all_topo)} problem instances, with average links {np.mean(all_topo)} and std {np.std(all_topo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
